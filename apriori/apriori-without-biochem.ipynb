{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import pickle\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_FILE = '../X_train_without_biochem.pkl'\n",
    "X_TEST_FILE = '../X_test_without_biochem.pkl'\n",
    "Y_TRAIN_FILE = '../y_train_without_biochem.pkl'\n",
    "Y_TEST_FILE = '../y_test_without_biochem.pkl'\n",
    "\n",
    "MIN_SUPPORT = 0.025\n",
    "MIN_THRESHOLD = 1.0\n",
    "\n",
    "APRIORI_FREQUENT_ITEMSETS_FILE_CSV = 'frequent_itemsets_without_biochem.csv'\n",
    "APRIORI_FREQUENT_ITEMSETS_FILE_PICKLE = 'frequent_itemsets_without_biochem.pkl'\n",
    "APRIORI_RULES_FILE_CSV = 'rules_without_biochem.csv'\n",
    "APRIORI_RULES_FILE_PICKLE = 'rules_without_biochem.pkl'\n",
    "APRIORI_OUTPUT_FILE_CSV = 'output_without_biochem.csv'\n",
    "APRIORI_OUTPUT_FILE_PICKLE = 'output_without_biochem.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dan/.local/share/virtualenvs/machine-learning-for-quality-improvement-NjV9ptfu/lib/python3.6/site-packages/ipykernel_launcher.py started at 2018-06-07 00:46:26.968330\n"
     ]
    }
   ],
   "source": [
    "script_start_time = datetime.datetime.now()\n",
    "print('{} started at {}'.format(sys.argv[0], script_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating rubric dictionary... done in 4.48s\n"
     ]
    }
   ],
   "source": [
    "# Generate the rubric dictionary\n",
    "print('Generating rubric dictionary...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "in_read_file = open('../input/large_anon_test_records_for_sharing.csv', mode='r')\n",
    "out_read_file = open('../input/large_anon_test_records_for_sharing.csv', mode='r')\n",
    "in_read_csv = csv.reader(in_read_file)\n",
    "out_read_csv = csv.reader(out_read_file)\n",
    "in_rubrics = {row[4]: row[3] for row in in_read_csv}\n",
    "out_rubrics = {row[4]: row[3] for row in out_read_csv}\n",
    "rubrics = {**in_rubrics, **out_rubrics}\n",
    "in_read_file.close()\n",
    "out_read_file.close()\n",
    "\n",
    "LABELS = ['ENTERING_INDICATOR', 'LEAVING_INDICATOR']\n",
    "\n",
    "def get_rubric(read_code):\n",
    "    if read_code in LABELS: \n",
    "        return read_code\n",
    "    \n",
    "    return rubrics.get(read_code, 'unknown')\n",
    "\n",
    "def is_entering_leaving(itemset):\n",
    "    return any(s in LABELS for s in itemset)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data... done in 0.03s\n"
     ]
    }
   ],
   "source": [
    "# Fetch all records first\n",
    "print('Reading data...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "X_train, X_test = pd.read_pickle(X_TRAIN_FILE), pd.read_pickle(X_TEST_FILE)\n",
    "y_train, y_test = pd.read_pickle(Y_TRAIN_FILE), pd.read_pickle(Y_TEST_FILE)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing... done in 0.04s\n"
     ]
    }
   ],
   "source": [
    "# Fetch all records first\n",
    "print('Preprocessing...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "train_df = pd.concat([X_train, y_train.rename('label')], axis=1)\n",
    "test_df = pd.concat([X_test, y_test.rename('label')], axis=1)\n",
    "merged_df = pd.concat([train_df, test_df], axis=0)\n",
    "merged_df['ENTERING_INDICATOR'] = merged_df.label.apply(lambda x: x)\n",
    "merged_df['LEAVING_INDICATOR'] = merged_df.label.apply(lambda x: not x)\n",
    "merged_df.drop('label', axis=1, inplace=True)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating frequent itemsets... done in 0.91s\n",
      "Frequent itemsets saved at frequent_itemsets_without_biochem.pkl and frequent_itemsets_without_biochem.csv\n"
     ]
    }
   ],
   "source": [
    "print('Calculating frequent itemsets...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "frequent_itemsets = apriori(merged_df, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)\n",
    "\n",
    "# Log to pickle and CSV\n",
    "with open(APRIORI_FREQUENT_ITEMSETS_FILE_PICKLE, mode='wb') as file:\n",
    "    pickle.dump(frequent_itemsets, file)\n",
    "frequent_itemsets.to_csv(APRIORI_FREQUENT_ITEMSETS_FILE_CSV)\n",
    "print('Frequent itemsets saved at {} and {}'.format(APRIORI_FREQUENT_ITEMSETS_FILE_PICKLE, APRIORI_FREQUENT_ITEMSETS_FILE_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating rules... done in 0.05s\n",
      "Converting Read codes to rubric...\n",
      " done in 0.01s\n",
      "Rules saved at rules_without_biochem.pkl and rules_without_biochem.csv\n",
      "Output saved at output_without_biochem.pkl and output_without_biochem.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/.local/share/virtualenvs/machine-learning-for-quality-improvement-NjV9ptfu/lib/python3.6/site-packages/pandas/core/frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "print('Generating rules...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=MIN_THRESHOLD)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)\n",
    "\n",
    "# Calculate length of each itemset in the antecedents and the consequents\n",
    "rules['antecedant length'] = rules['antecedants'].apply(lambda x: len(x))\n",
    "rules['consequent length'] = rules['consequents'].apply(lambda x: len(x))\n",
    "\n",
    "# Filter itemsets to only include those with 2 or more items\n",
    "rules = rules.loc[(rules['antecedant length'] >= 2) | (rules['consequent length'] >= 2), :]\n",
    "\n",
    "rules.drop(['antecedant length', 'consequent length'], axis='columns', inplace=True)\n",
    "\n",
    "# Add rubric for easier interpretation\n",
    "print('Converting Read codes to rubric...')\n",
    "start_time = timeit.default_timer()\n",
    "rules.drop(['antecedent support', 'consequent support', 'leverage', 'conviction'], axis='columns', inplace=True)\n",
    "rules['antecedants rubric'] = rules['antecedants'].apply(lambda itemset: [get_rubric(item) for item in itemset])\n",
    "rules['consequents rubric'] = rules['consequents'].apply(lambda itemset: [get_rubric(item) for item in itemset])\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)\n",
    "\n",
    "# Sort by lift\n",
    "rules.sort_values('lift', ascending=False, inplace=True)\n",
    "\n",
    "# Log to pickle and CSV\n",
    "with open(APRIORI_RULES_FILE_PICKLE, mode='wb') as file:\n",
    "    pickle.dump(rules, file)\n",
    "rules.to_csv(APRIORI_RULES_FILE_CSV)\n",
    "print('Rules saved at {} and {}'.format(APRIORI_RULES_FILE_PICKLE, APRIORI_RULES_FILE_CSV))\n",
    "\n",
    "# Filter to those rules containing a label in LABELS\n",
    "is_antecedent_entering_leaving = rules.antecedants.apply(is_entering_leaving)\n",
    "is_consequent_entering_leaving = rules.consequents.apply(is_entering_leaving)\n",
    "\n",
    "output_df = rules.loc[(is_antecedent_entering_leaving) | (is_consequent_entering_leaving), :]\n",
    "\n",
    "# Drop the antecedants and consequents columns\n",
    "output_df.drop(['antecedants', 'consequents'], axis='columns', inplace=True)\n",
    "\n",
    "with open(APRIORI_OUTPUT_FILE_PICKLE, mode='wb') as file:\n",
    "    pickle.dump(output_df, file)\n",
    "output_df.to_csv(APRIORI_OUTPUT_FILE_CSV)\n",
    "print('Output saved at {} and {}'.format(APRIORI_OUTPUT_FILE_PICKLE, APRIORI_OUTPUT_FILE_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dan/.local/share/virtualenvs/machine-learning-for-quality-improvement-NjV9ptfu/lib/python3.6/site-packages/ipykernel_launcher.py completed at 2018-06-07 00:46:32.882789\n",
      "Total time: 0:00:05.914459\n"
     ]
    }
   ],
   "source": [
    "script_end_time = datetime.datetime.now()\n",
    "print('{} completed at {}'.format(\n",
    "    sys.argv[0], \n",
    "    script_end_time)\n",
    ")\n",
    "print('Total time: {}'.format(script_end_time - script_start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
