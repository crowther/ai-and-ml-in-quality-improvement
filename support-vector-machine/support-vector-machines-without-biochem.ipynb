{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import csv\n",
    "import timeit\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_FILE = '../X_train_without_biochem.pkl'\n",
    "X_TEST_FILE = '../X_test_without_biochem.pkl'\n",
    "Y_TRAIN_FILE = '../y_train_without_biochem.pkl'\n",
    "Y_TEST_FILE = '../y_test_without_biochem.pkl'\n",
    "METRICS_FILE = 'metrics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\richard\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py started at 2018-06-05 15:37:08.169414\n"
     ]
    }
   ],
   "source": [
    "script_start_time = datetime.datetime.now()\n",
    "print('{} started at {}'.format(sys.argv[0], script_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the rubric dictionary\n",
    "print('Generating rubric dictionary...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "in_read_file = open('../input/large_anon_test_records_for_sharing.csv', mode='r')\n",
    "out_read_file = open('../input/large_anon_test_records_for_sharing.csv', mode='r')\n",
    "in_read_csv = csv.reader(in_read_file)\n",
    "out_read_csv = csv.reader(out_read_file)\n",
    "in_rubrics = {row[4]: row[3] for row in in_read_csv}\n",
    "out_rubrics = {row[4]: row[3] for row in out_read_csv}\n",
    "rubrics = {**in_rubrics, **out_rubrics}\n",
    "in_read_file.close()\n",
    "out_read_file.close()\n",
    "\n",
    "def get_rubric(read_code):    \n",
    "    return rubrics.get(read_code, 'unknown')\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data... done in 0.05s\n",
      "False    502\n",
      "True     409\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all patients for train-test splitting\n",
    "print('Reading data...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "X_train, X_test = pd.read_pickle(X_TRAIN_FILE), pd.read_pickle(X_TEST_FILE)\n",
    "y_train, y_test = pd.read_pickle(Y_TRAIN_FILE), pd.read_pickle(Y_TEST_FILE)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search and training model...Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[Parallel(n_jobs=16)]: Done   1 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=16)]: Done   3 out of   3 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done   3 out of   3 | elapsed:  1.2min finished\n",
      " done in 128.21s\n",
      "The best parameters are {'C': 1.0, 'kernel': 'linear'} with a score of 0.49366064202859455\n",
      "Making predictions... done in 13.31s\n",
      "Accuracy: 54.12\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      False       0.58      0.62      0.60       502\n",
      "       True       0.49      0.44      0.46       409\n",
      "\n",
      "avg / total       0.54      0.54      0.54       911\n",
      "\n",
      "Confusion matrix:\n",
      "  [[313 189]\n",
      " [229 180]]\n",
      "Metrics saved at metrics.txt\n"
     ]
    }
   ],
   "source": [
    "print('Performing grid search and training model...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "# param_grid = [\n",
    "#     {'C': np.logspace(-2, 10, 13), 'kernel': ['linear']}\n",
    "# ]\n",
    "param_grid = [\n",
    "    {'C': [1.00], 'kernel': ['linear']}\n",
    "]\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, n_jobs=16, verbose=50, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)\n",
    "print('The best parameters are {} with a score of {}'.format(grid.best_params_, grid.best_score_))\n",
    "\n",
    "print('Making predictions...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "train_preds = grid.predict(X_train)\n",
    "test_preds = grid.predict(X_test)\n",
    "print(' done in {:.2f}s'.format(timeit.default_timer() - start_time), flush=True)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Train accuracy: {:.2f} Test accuracy: {:.2f}'.format(train_accuracy * 100, test_accuracy * 100.0))\n",
    "\n",
    "classification_report = classification_report(y_test, test_preds)\n",
    "print('Classification report:\\n ', classification_report)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, test_preds)\n",
    "print('Confusion matrix:\\n ', confusion_matrix)\n",
    "\n",
    "with open(METRICS_FILE, mode='w') as file:\n",
    "    file.write('Train accuracy: {:.2f} Test accuracy: {:.2f}\\n'.format(train_accuracy * 100, test_accuracy * 100.0))\n",
    "    file.write('Classification report: {}\\n'.format(classification_report))\n",
    "    file.write('Confusion matrix: {}\\n'.format(confusion_matrix))\n",
    "    file.write('Cross validation results: {}\\n'.format(grid.cv_results_))\n",
    "print('Metrics saved at {}'.format(METRICS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting importance..."
     ]
    }
   ],
   "source": [
    "print('Sorting importance...', end='')\n",
    "start_time = timeit.default_timer()\n",
    "svm = grid.best_estimator_\n",
    "feature_dict = {get_rubric(f): i for f, i in zip(X_test.columns, *svm.coef_)}\n",
    "\n",
    "importances = pd.DataFrame.from_dict(feature_dict, orient='index').rename(columns={0: 'importance'})\n",
    "\n",
    "importances.sort_values(by='importance', inplace=True, ascending=False)\n",
    "importances.to_csv('importances_without_biochem.csv')\n",
    "importances.to_pickle('importances_without_biochem.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\richard\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py completed at 2018-06-05 15:39:30.343357\n",
      "Total time: 0:02:22.173943\n"
     ]
    }
   ],
   "source": [
    "script_end_time = datetime.datetime.now()\n",
    "print('{} completed at {}'.format(\n",
    "    sys.argv[0], \n",
    "    script_end_time)\n",
    ")\n",
    "print('Total time: {}'.format(script_end_time - script_start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
